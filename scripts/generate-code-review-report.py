#!/usr/bin/env python3
"""
Script de Automa√ß√£o de Relat√≥rio de Code Review
================================================

Este script automatiza o preenchimento das se√ß√µes do template de Code Review:
- Informa√ß√µes da Revis√£o (via GitHub API)
- M√©tricas de Qualidade (via Lighthouse CLI e axe-core)
- An√°lise est√°tica de acessibilidade

Uso:
    python3 scripts/generate-code-review-report.py --pr 123
    python3 scripts/generate-code-review-report.py --pr 123 --run-lighthouse
    python3 scripts/generate-code-review-report.py --pr 123 --full-analysis
    python3 scripts/generate-code-review-report.py --local --files src/components/LoginForm.tsx

Requisitos:
    - gh CLI instalado e autenticado
    - Node.js para Lighthouse (opcional)
    - axe-core para an√°lise de acessibilidade (opcional)

Autor: TSiJUKEBOX Team
Vers√£o: 1.0.0
Data: 2025-12-25
"""

import os
import re
import json
import argparse
import subprocess
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Optional, Tuple
import tempfile


# =============================================================================
# CONFIGURA√á√ïES
# =============================================================================

BASE_DIR = Path(__file__).parent.parent
TEMPLATE_PATH = BASE_DIR / "docs" / "templates" / "CODE_REVIEW_REPORT_TEMPLATE.md"
OUTPUT_DIR = BASE_DIR / "docs" / "reviews"
CHECKLIST_PATH = BASE_DIR / "docs" / "accessibility" / "HYBRID_PATTERN_CODE_REVIEW_CHECKLIST.md"

# GitHub
GITHUB_REPO = os.environ.get("GITHUB_REPOSITORY", "")
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN", "")

# Lighthouse
LIGHTHOUSE_URL = "http://localhost:5173"  # URL do dev server
LIGHTHOUSE_CATEGORIES = ["accessibility"]

# Thresholds
SCORE_APPROVED = 95
SCORE_WITH_NOTES = 85
SCORE_CHANGES_REQUESTED = 70


# =============================================================================
# ESTRUTURAS DE DADOS
# =============================================================================

@dataclass
class PRInfo:
    """Informa√ß√µes do Pull Request."""
    number: int = 0
    title: str = ""
    author: str = ""
    branch: str = ""
    base_branch: str = ""
    created_at: str = ""
    files_changed: List[str] = field(default_factory=list)
    additions: int = 0
    deletions: int = 0
    url: str = ""


@dataclass
class LighthouseMetrics:
    """M√©tricas do Lighthouse."""
    accessibility_score: int = 0
    performance_score: int = 0
    best_practices_score: int = 0
    seo_score: int = 0
    accessibility_issues: List[Dict] = field(default_factory=list)


@dataclass
class AxeMetrics:
    """M√©tricas do axe-core."""
    violations: int = 0
    passes: int = 0
    incomplete: int = 0
    inapplicable: int = 0
    violation_details: List[Dict] = field(default_factory=list)


@dataclass
class StaticAnalysisMetrics:
    """M√©tricas de an√°lise est√°tica."""
    aria_invalid_count: int = 0
    aria_label_count: int = 0
    aria_describedby_count: int = 0
    role_alert_count: int = 0
    form_count: int = 0
    input_count: int = 0
    label_count: int = 0
    issues: List[Dict] = field(default_factory=list)


@dataclass
class ChecklistScore:
    """Pontua√ß√£o do checklist."""
    total: int = 96
    approved: int = 0
    attention: int = 0
    rejected: int = 0
    na: int = 0
    score_percentage: float = 0.0
    by_category: Dict[str, Dict] = field(default_factory=dict)


@dataclass
class ReviewReport:
    """Relat√≥rio completo de Code Review."""
    pr_info: PRInfo = field(default_factory=PRInfo)
    lighthouse: LighthouseMetrics = field(default_factory=LighthouseMetrics)
    axe: AxeMetrics = field(default_factory=AxeMetrics)
    static_analysis: StaticAnalysisMetrics = field(default_factory=StaticAnalysisMetrics)
    checklist_score: ChecklistScore = field(default_factory=ChecklistScore)
    reviewer: str = ""
    review_date: str = ""
    form_type: str = ""
    pattern_type: str = ""
    complexity: str = ""
    decision: str = ""
    generated_at: str = ""


# =============================================================================
# FUN√á√ïES DE COLETA DE DADOS
# =============================================================================

def get_pr_info(pr_number: int) -> PRInfo:
    """Obt√©m informa√ß√µes do PR via GitHub CLI."""
    pr_info = PRInfo(number=pr_number)
    
    try:
        # Obter dados b√°sicos do PR
        result = subprocess.run(
            ["gh", "pr", "view", str(pr_number), "--json",
             "title,author,headRefName,baseRefName,createdAt,additions,deletions,url,files"],
            capture_output=True,
            text=True,
            cwd=BASE_DIR
        )
        
        if result.returncode == 0:
            data = json.loads(result.stdout)
            pr_info.title = data.get("title", "")
            pr_info.author = data.get("author", {}).get("login", "")
            pr_info.branch = data.get("headRefName", "")
            pr_info.base_branch = data.get("baseRefName", "")
            pr_info.created_at = data.get("createdAt", "")[:10]
            pr_info.additions = data.get("additions", 0)
            pr_info.deletions = data.get("deletions", 0)
            pr_info.url = data.get("url", "")
            pr_info.files_changed = [f.get("path", "") for f in data.get("files", [])]
        else:
            print(f"‚ö†Ô∏è Erro ao obter PR: {result.stderr}")
            
    except FileNotFoundError:
        print("‚ö†Ô∏è GitHub CLI (gh) n√£o encontrado. Instale com: sudo apt install gh")
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao obter informa√ß√µes do PR: {e}")
    
    return pr_info


def get_current_user() -> str:
    """Obt√©m o usu√°rio atual do GitHub."""
    try:
        result = subprocess.run(
            ["gh", "api", "user", "--jq", ".login"],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            return result.stdout.strip()
    except:
        pass
    return os.environ.get("USER", "reviewer")


def run_lighthouse(url: str = LIGHTHOUSE_URL) -> LighthouseMetrics:
    """Executa Lighthouse e retorna m√©tricas de acessibilidade."""
    metrics = LighthouseMetrics()
    
    try:
        with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as tmp:
            tmp_path = tmp.name
        
        # Executar Lighthouse
        result = subprocess.run(
            [
                "npx", "lighthouse", url,
                "--output=json",
                f"--output-path={tmp_path}",
                "--only-categories=accessibility",
                "--chrome-flags=--headless --no-sandbox",
                "--quiet"
            ],
            capture_output=True,
            text=True,
            timeout=120
        )
        
        if result.returncode == 0 and Path(tmp_path).exists():
            with open(tmp_path, 'r') as f:
                data = json.load(f)
            
            categories = data.get("categories", {})
            metrics.accessibility_score = int(
                categories.get("accessibility", {}).get("score", 0) * 100
            )
            
            # Extrair issues de acessibilidade
            audits = data.get("audits", {})
            for audit_id, audit in audits.items():
                if audit.get("score") == 0:
                    metrics.accessibility_issues.append({
                        "id": audit_id,
                        "title": audit.get("title", ""),
                        "description": audit.get("description", ""),
                    })
            
            Path(tmp_path).unlink()
        else:
            print(f"‚ö†Ô∏è Lighthouse falhou: {result.stderr[:200]}")
            
    except subprocess.TimeoutExpired:
        print("‚ö†Ô∏è Lighthouse timeout (120s)")
    except FileNotFoundError:
        print("‚ö†Ô∏è Lighthouse n√£o encontrado. Instale com: npm install -g lighthouse")
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao executar Lighthouse: {e}")
    
    return metrics


def run_axe_analysis(url: str = LIGHTHOUSE_URL) -> AxeMetrics:
    """Executa an√°lise axe-core via Puppeteer."""
    metrics = AxeMetrics()
    
    # Script Node.js para executar axe-core
    axe_script = """
const puppeteer = require('puppeteer');
const { AxePuppeteer } = require('@axe-core/puppeteer');

(async () => {
    const browser = await puppeteer.launch({
        headless: 'new',
        args: ['--no-sandbox']
    });
    const page = await browser.newPage();
    await page.goto(process.argv[2], { waitUntil: 'networkidle0' });
    
    const results = await new AxePuppeteer(page).analyze();
    
    console.log(JSON.stringify({
        violations: results.violations.length,
        passes: results.passes.length,
        incomplete: results.incomplete.length,
        inapplicable: results.inapplicable.length,
        violation_details: results.violations.map(v => ({
            id: v.id,
            impact: v.impact,
            description: v.description,
            nodes: v.nodes.length
        }))
    }));
    
    await browser.close();
})();
"""
    
    try:
        with tempfile.NamedTemporaryFile(suffix=".js", delete=False, mode='w') as tmp:
            tmp.write(axe_script)
            tmp_path = tmp.name
        
        result = subprocess.run(
            ["node", tmp_path, url],
            capture_output=True,
            text=True,
            timeout=60
        )
        
        if result.returncode == 0:
            data = json.loads(result.stdout)
            metrics.violations = data.get("violations", 0)
            metrics.passes = data.get("passes", 0)
            metrics.incomplete = data.get("incomplete", 0)
            metrics.inapplicable = data.get("inapplicable", 0)
            metrics.violation_details = data.get("violation_details", [])
        
        Path(tmp_path).unlink()
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao executar axe-core: {e}")
    
    return metrics


def run_static_analysis(files: List[str]) -> StaticAnalysisMetrics:
    """Executa an√°lise est√°tica nos arquivos."""
    metrics = StaticAnalysisMetrics()
    
    patterns = {
        'aria_invalid': r'aria-invalid=',
        'aria_label': r'aria-label=',
        'aria_describedby': r'aria-describedby=',
        'role_alert': r'role="alert"',
        'form': r'<form\b',
        'input': r'<(?:Input|input)\b',
        'label': r'<(?:Label|label)\b',
    }
    
    issue_patterns = [
        (r'<(?:Input|input)[^>]*(?!aria-label)[^>]*placeholder="[^"]+"[^>]*/?>',
         "Input com placeholder sem aria-label", "HIGH"),
        (r'<(?:Input|input)[^>]*(?!aria-invalid)[^>]*className="[^"]*error[^"]*"',
         "Input com classe de erro sem aria-invalid", "HIGH"),
        (r'error\s*&&\s*<(?:span|p|div)(?![^>]*role="alert")',
         "Mensagem de erro sem role='alert'", "MEDIUM"),
        (r'<(?:Input|input)[^>]*required(?![^>]*aria-required)',
         "Campo required sem aria-required", "LOW"),
    ]
    
    for filepath in files:
        full_path = BASE_DIR / filepath if not Path(filepath).is_absolute() else Path(filepath)
        
        if not full_path.exists() or full_path.suffix not in ['.tsx', '.jsx']:
            continue
        
        try:
            content = full_path.read_text()
            
            # Contar padr√µes
            for key, pattern in patterns.items():
                count = len(re.findall(pattern, content, re.IGNORECASE))
                setattr(metrics, f"{key}_count", getattr(metrics, f"{key}_count") + count)
            
            # Detectar issues
            lines = content.split('\n')
            for i, line in enumerate(lines):
                for pattern, description, severity in issue_patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        metrics.issues.append({
                            "file": str(filepath),
                            "line": i + 1,
                            "description": description,
                            "severity": severity,
                            "context": line.strip()[:80]
                        })
                        
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao analisar {filepath}: {e}")
    
    return metrics


def calculate_checklist_score(static_metrics: StaticAnalysisMetrics) -> ChecklistScore:
    """Calcula pontua√ß√£o estimada do checklist baseado na an√°lise est√°tica."""
    score = ChecklistScore()
    
    # Estimativas baseadas em m√©tricas
    categories = {
        "Estrutura do Formul√°rio": {
            "total": 7,
            "estimated": min(7, static_metrics.form_count * 2) if static_metrics.form_count > 0 else 0
        },
        "Resumo de Erros": {
            "total": 15,
            "estimated": 10 if static_metrics.role_alert_count > 0 else 5
        },
        "Erros Inline": {
            "total": 13,
            "estimated": min(13, (
                (3 if static_metrics.aria_invalid_count > 0 else 0) +
                (3 if static_metrics.aria_describedby_count > 0 else 0) +
                (3 if static_metrics.role_alert_count > 0 else 0) +
                4
            ))
        },
        "Labels e Associa√ß√µes": {
            "total": 8,
            "estimated": min(8, (
                (4 if static_metrics.label_count >= static_metrics.input_count else 2) +
                (2 if static_metrics.aria_label_count > 0 else 0) +
                2
            ))
        },
        "Valida√ß√£o e Timing": {
            "total": 8,
            "estimated": 6  # Dif√≠cil estimar estaticamente
        },
        "Feedback Visual": {
            "total": 10,
            "estimated": 7  # Dif√≠cil estimar estaticamente
        },
        "Autocomplete e Tipos": {
            "total": 8,
            "estimated": 5  # Dif√≠cil estimar estaticamente
        },
        "Navega√ß√£o por Teclado": {
            "total": 7,
            "estimated": 5  # Dif√≠cil estimar estaticamente
        },
        "Testes Manuais": {
            "total": 12,
            "estimated": 0  # Requer testes manuais
        },
        "C√≥digo Limpo": {
            "total": 8,
            "estimated": 6  # Dif√≠cil estimar estaticamente
        },
    }
    
    total_estimated = 0
    total_possible = 0
    
    for category, data in categories.items():
        score.by_category[category] = {
            "total": data["total"],
            "estimated": data["estimated"],
            "percentage": (data["estimated"] / data["total"] * 100) if data["total"] > 0 else 0
        }
        total_estimated += data["estimated"]
        total_possible += data["total"]
    
    # Penalizar por issues encontrados
    penalty = len(static_metrics.issues) * 2
    total_estimated = max(0, total_estimated - penalty)
    
    score.approved = total_estimated
    score.attention = min(10, len(static_metrics.issues))
    score.rejected = max(0, len([i for i in static_metrics.issues if i["severity"] == "HIGH"]))
    score.total = total_possible
    score.score_percentage = (total_estimated / total_possible * 100) if total_possible > 0 else 0
    
    return score


def determine_decision(score: ChecklistScore, lighthouse: LighthouseMetrics, axe: AxeMetrics) -> str:
    """Determina a decis√£o baseada nas m√©tricas."""
    # Crit√©rios de bloqueio
    if axe.violations > 5:
        return "‚ùå REPROVADO"
    
    if lighthouse.accessibility_score < 70:
        return "üîÑ SOLICITAR ALTERA√á√ïES"
    
    # Baseado no score do checklist
    if score.score_percentage >= SCORE_APPROVED and score.rejected == 0:
        return "‚úÖ APROVADO"
    elif score.score_percentage >= SCORE_WITH_NOTES:
        return "‚ö†Ô∏è APROVADO COM RESSALVAS"
    elif score.score_percentage >= SCORE_CHANGES_REQUESTED:
        return "üîÑ SOLICITAR ALTERA√á√ïES"
    else:
        return "‚ùå REPROVADO"


# =============================================================================
# GERA√á√ÉO DO RELAT√ìRIO
# =============================================================================

def generate_report_markdown(report: ReviewReport) -> str:
    """Gera o relat√≥rio em formato Markdown."""
    
    # Carregar template
    if TEMPLATE_PATH.exists():
        template = TEMPLATE_PATH.read_text()
    else:
        template = get_default_template()
    
    # Substituir informa√ß√µes da revis√£o
    replacements = {
        # Informa√ß√µes da Revis√£o
        "| **PR/MR #** | |": f"| **PR/MR #** | #{report.pr_info.number} |",
        "| **T√≠tulo** | |": f"| **T√≠tulo** | {report.pr_info.title} |",
        "| **Autor** | |": f"| **Autor** | @{report.pr_info.author} |",
        "| **Revisor** | |": f"| **Revisor** | @{report.reviewer} |",
        "| **Data da Revis√£o** | |": f"| **Data da Revis√£o** | {report.review_date} |",
        "| **Branch** | |": f"| **Branch** | `{report.pr_info.branch}` ‚Üí `{report.pr_info.base_branch}` |",
        "| **Arquivos Alterados** | |": f"| **Arquivos Alterados** | {len(report.pr_info.files_changed)} (+{report.pr_info.additions}/-{report.pr_info.deletions}) |",
        
        # M√©tricas de Qualidade
        "| **Score do Checklist** | % |": f"| **Score do Checklist** | {report.checklist_score.score_percentage:.1f}% |",
        "| **Itens Reprovados** | |": f"| **Itens Reprovados** | {report.checklist_score.rejected} |",
        "| **Itens Bloqueantes** | |": f"| **Itens Bloqueantes** | {len([i for i in report.static_analysis.issues if i['severity'] == 'HIGH'])} |",
        "| **Lighthouse Accessibility** | |": f"| **Lighthouse Accessibility** | {report.lighthouse.accessibility_score} |",
        "| **axe-core Violations** | |": f"| **axe-core Violations** | {report.axe.violations} |",
        
        # Score
        "| **Score Bruto** | /96 |": f"| **Score Bruto** | {report.checklist_score.approved}/96 |",
        "| **Itens N/A** | |": f"| **Itens N/A** | {report.checklist_score.na} |",
        "| **Score Ajustado** | % |": f"| **Score Ajustado** | {report.checklist_score.score_percentage:.1f}% |",
    }
    
    result = template
    for old, new in replacements.items():
        result = result.replace(old, new)
    
    # Adicionar se√ß√£o de an√°lise est√°tica
    static_section = generate_static_analysis_section(report.static_analysis)
    result = result.replace(
        "## ‚úÖ Pontos Positivos",
        f"{static_section}\n\n## ‚úÖ Pontos Positivos"
    )
    
    # Adicionar issues encontrados
    if report.static_analysis.issues:
        issues_section = generate_issues_section(report.static_analysis.issues)
        result = result.replace(
            "## ‚ùå Corre√ß√µes Obrigat√≥rias\n\nListe os aspectos que DEVEM ser corrigidos antes do merge:",
            f"## ‚ùå Corre√ß√µes Obrigat√≥rias\n\n{issues_section}"
        )
    
    # Marcar decis√£o
    decision_markers = {
        "‚úÖ APROVADO": "- [x] ‚úÖ **APROVADO**",
        "‚ö†Ô∏è APROVADO COM RESSALVAS": "- [x] ‚ö†Ô∏è **APROVADO COM RESSALVAS**",
        "üîÑ SOLICITAR ALTERA√á√ïES": "- [x] üîÑ **SOLICITAR ALTERA√á√ïES**",
        "‚ùå REPROVADO": "- [x] ‚ùå **REPROVADO**",
    }
    
    for decision, marker in decision_markers.items():
        if decision in report.decision:
            result = result.replace(f"- [ ] {decision}", marker)
    
    # Adicionar timestamp
    result = result.replace(
        "*Gerado em: 2025-12-25*",
        f"*Gerado automaticamente em: {report.generated_at}*"
    )
    
    return result


def generate_static_analysis_section(metrics: StaticAnalysisMetrics) -> str:
    """Gera se√ß√£o de an√°lise est√°tica."""
    return f"""
## üîç An√°lise Est√°tica Autom√°tica

### M√©tricas de Acessibilidade Detectadas

| Atributo | Quantidade | Status |
|----------|------------|--------|
| `aria-invalid` | {metrics.aria_invalid_count} | {'‚úÖ' if metrics.aria_invalid_count > 0 else '‚ö†Ô∏è'} |
| `aria-label` | {metrics.aria_label_count} | {'‚úÖ' if metrics.aria_label_count > 0 else '‚ö†Ô∏è'} |
| `aria-describedby` | {metrics.aria_describedby_count} | {'‚úÖ' if metrics.aria_describedby_count > 0 else '‚ö†Ô∏è'} |
| `role="alert"` | {metrics.role_alert_count} | {'‚úÖ' if metrics.role_alert_count > 0 else '‚ö†Ô∏è'} |
| `<form>` | {metrics.form_count} | {'‚úÖ' if metrics.form_count > 0 else '‚ûñ'} |
| `<Input>` | {metrics.input_count} | {'‚úÖ' if metrics.input_count > 0 else '‚ûñ'} |
| `<Label>` | {metrics.label_count} | {'‚úÖ' if metrics.label_count >= metrics.input_count else '‚ö†Ô∏è'} |

### Cobertura de Labels

{'‚úÖ Todos os inputs possuem labels associados' if metrics.label_count >= metrics.input_count else f'‚ö†Ô∏è {metrics.input_count - metrics.label_count} inputs podem estar sem label'}
"""


def generate_issues_section(issues: List[Dict]) -> str:
    """Gera se√ß√£o de issues encontrados."""
    if not issues:
        return "Nenhuma corre√ß√£o obrigat√≥ria identificada automaticamente."
    
    lines = ["Issues identificados automaticamente:\n"]
    lines.append("| # | Descri√ß√£o | Arquivo:Linha | Severidade | Bloqueante |")
    lines.append("|---|-----------|---------------|------------|------------|")
    
    for i, issue in enumerate(issues[:10], 1):
        bloqueante = "‚òëÔ∏è Sim" if issue["severity"] == "HIGH" else "‚òê N√£o"
        lines.append(
            f"| {i} | {issue['description']} | `{issue['file']}:{issue['line']}` | {issue['severity']} | {bloqueante} |"
        )
    
    if len(issues) > 10:
        lines.append(f"\n*... e mais {len(issues) - 10} issues*")
    
    return "\n".join(lines)


def get_default_template() -> str:
    """Retorna template padr√£o caso o arquivo n√£o exista."""
    return """# üìã Relat√≥rio de Code Review - Acessibilidade de Formul√°rios

## üìå Informa√ß√µes da Revis√£o

| Campo | Valor |
|-------|-------|
| **PR/MR #** | |
| **T√≠tulo** | |
| **Autor** | |
| **Revisor** | |
| **Data da Revis√£o** | |
| **Branch** | |
| **Arquivos Alterados** | |

## üìä M√©tricas de Qualidade

| M√©trica | Valor | Meta | Status |
|---------|-------|------|--------|
| **Score do Checklist** | % | ‚â• 90% | ‚òê |
| **Itens Reprovados** | | 0 | ‚òê |
| **Itens Bloqueantes** | | 0 | ‚òê |
| **Lighthouse Accessibility** | | ‚â• 90 | ‚òê |
| **axe-core Violations** | | 0 | ‚òê |

## ‚úÖ Pontos Positivos

## ‚ùå Corre√ß√µes Obrigat√≥rias

Liste os aspectos que DEVEM ser corrigidos antes do merge:

## üéØ Decis√£o Final

- [ ] ‚úÖ **APROVADO**
- [ ] ‚ö†Ô∏è **APROVADO COM RESSALVAS**
- [ ] üîÑ **SOLICITAR ALTERA√á√ïES**
- [ ] ‚ùå **REPROVADO**

*Gerado em: 2025-12-25*
"""


# =============================================================================
# INTERFACE DE LINHA DE COMANDO
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='Gera relat√≥rio de Code Review automatizado para formul√°rios acess√≠veis',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos:
  python3 generate-code-review-report.py --pr 123
  python3 generate-code-review-report.py --pr 123 --run-lighthouse
  python3 generate-code-review-report.py --local --files src/components/LoginForm.tsx
        """
    )
    
    parser.add_argument('--pr', type=int, help='N√∫mero do Pull Request')
    parser.add_argument('--local', action='store_true', help='An√°lise local sem PR')
    parser.add_argument('--files', nargs='+', help='Arquivos para an√°lise local')
    parser.add_argument('--run-lighthouse', action='store_true', help='Executar Lighthouse')
    parser.add_argument('--run-axe', action='store_true', help='Executar axe-core')
    parser.add_argument('--full-analysis', action='store_true', help='Executar todas as an√°lises')
    parser.add_argument('--url', default=LIGHTHOUSE_URL, help='URL para Lighthouse/axe')
    parser.add_argument('--output', type=str, help='Caminho do arquivo de sa√≠da')
    parser.add_argument('--reviewer', type=str, help='Nome do revisor')
    
    args = parser.parse_args()
    
    # Validar argumentos
    if not args.pr and not args.local:
        parser.error("Especifique --pr <n√∫mero> ou --local")
    
    if args.local and not args.files:
        parser.error("--local requer --files")
    
    if args.full_analysis:
        args.run_lighthouse = True
        args.run_axe = True
    
    # Inicializar relat√≥rio
    report = ReviewReport()
    report.review_date = datetime.now().strftime("%Y-%m-%d")
    report.generated_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    report.reviewer = args.reviewer or get_current_user()
    
    print("=" * 70)
    print("üìã Gerador de Relat√≥rio de Code Review")
    print("=" * 70)
    print(f"‚è±Ô∏è In√≠cio: {datetime.now().strftime('%H:%M:%S')}")
    print()
    
    # Coletar informa√ß√µes do PR
    if args.pr:
        print(f"üîç Obtendo informa√ß√µes do PR #{args.pr}...")
        report.pr_info = get_pr_info(args.pr)
        files_to_analyze = report.pr_info.files_changed
        print(f"   ‚úÖ {len(files_to_analyze)} arquivos alterados")
    else:
        files_to_analyze = args.files
        report.pr_info.title = "An√°lise Local"
        report.pr_info.files_changed = files_to_analyze
    
    # An√°lise est√°tica
    print(f"\nüî¨ Executando an√°lise est√°tica...")
    report.static_analysis = run_static_analysis(files_to_analyze)
    print(f"   ‚úÖ {len(report.static_analysis.issues)} issues encontrados")
    
    # Lighthouse
    if args.run_lighthouse:
        print(f"\nüî¶ Executando Lighthouse ({args.url})...")
        report.lighthouse = run_lighthouse(args.url)
        print(f"   ‚úÖ Score de acessibilidade: {report.lighthouse.accessibility_score}")
    
    # axe-core
    if args.run_axe:
        print(f"\nü™ì Executando axe-core ({args.url})...")
        report.axe = run_axe_analysis(args.url)
        print(f"   ‚úÖ Violations: {report.axe.violations}")
    
    # Calcular score
    print(f"\nüìä Calculando score do checklist...")
    report.checklist_score = calculate_checklist_score(report.static_analysis)
    print(f"   ‚úÖ Score estimado: {report.checklist_score.score_percentage:.1f}%")
    
    # Determinar decis√£o
    report.decision = determine_decision(
        report.checklist_score,
        report.lighthouse,
        report.axe
    )
    print(f"\nüéØ Decis√£o: {report.decision}")
    
    # Gerar relat√≥rio
    print(f"\nüìù Gerando relat√≥rio...")
    markdown = generate_report_markdown(report)
    
    # Salvar arquivo
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    if args.output:
        output_path = Path(args.output)
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        pr_suffix = f"_PR{args.pr}" if args.pr else "_local"
        output_path = OUTPUT_DIR / f"code_review{pr_suffix}_{timestamp}.md"
    
    output_path.write_text(markdown, encoding='utf-8')
    
    print(f"   ‚úÖ Relat√≥rio salvo em: {output_path}")
    
    # Resumo final
    print()
    print("=" * 70)
    print("üìä RESUMO")
    print("-" * 70)
    print(f"   PR: #{report.pr_info.number} - {report.pr_info.title[:50]}")
    print(f"   Arquivos: {len(files_to_analyze)}")
    print(f"   Issues: {len(report.static_analysis.issues)}")
    print(f"   Score: {report.checklist_score.score_percentage:.1f}%")
    print(f"   Lighthouse: {report.lighthouse.accessibility_score}")
    print(f"   axe-core: {report.axe.violations} violations")
    print(f"   Decis√£o: {report.decision}")
    print("=" * 70)
    print(f"‚è±Ô∏è Fim: {datetime.now().strftime('%H:%M:%S')}")
    
    # Retornar c√≥digo de sa√≠da baseado na decis√£o
    if "REPROVADO" in report.decision:
        return 2
    elif "ALTERA√á√ïES" in report.decision:
        return 1
    return 0


if __name__ == '__main__':
    exit(main())
